# <center>æ·±åº¦å­¸ç¿’æ‡‰ç”¨ - ä½œæ¥­ä¸€</center>

##### <center>ç³»ç´šï¼šé›»æ©Ÿæ‰€ç¢©ä¸€ &emsp; å§“åï¼šæ¥Šå† å½¥ &emsp; å­¸è™Ÿï¼šR11921091 </center>

## Q1: Data processing
æˆ‘ä½¿ç”¨ sample code ã€‚

a. é‡å° intent åŠ slot æˆ‘éƒ½ä½¿ç”¨ sample code çš„æ–¹å¼ï¼Œæ ¹æ“šç©ºæ ¼å»åˆ†éš”æ–‡å­—ï¼Œä¸¦ä¾æ“šæ¯å€‹ words å‡ºç¾çš„é »ç‡å»ºæˆå­—å…¸ã€‚

b. åœ¨é€™å€‹ä½œæ¥­ä¸­ï¼Œæˆ‘ç›´æ¥ä½¿ç”¨äº† token dictionary å’Œ Glove çš„ [glove.840B.300d](https://nlp.stanford.edu/data/glove.840B.300d.zip) çš„ pre-trained embedding ã€‚é€™å€‹ pre-trained embedding ç‚º 840B tokensã€2.2M vocabã€300ç¶­åº¦å‘é‡ä¸”æœ‰å€åˆ†å¤§å°å¯«çš„ embedding ã€‚å¦æ ¹æ“š preprocess.sh å›å‚³çš„çµ±è¨ˆçµæœï¼Œç´„æœ‰ 83% çš„ token åœ¨ intent classification ä¸­è¢« GloVe æ¶µè“‹ï¼Œåœ¨ slot classification ä¸­å‰‡æ¶µè“‹äº† 72%ã€‚

## Q2: Describe your intent classification model.
### a. model
æˆ‘å°‡æˆ‘çš„æ¨¡å‹å€åˆ†æˆembedding layerã€bi-LSTMã€Fully-connected layer ä¸‰å±¤ã€‚ç¶“éå‰è¿°Data processingçš„tokenizeræœƒè¼¸å‡ºlong type çš„é•·åº¦ç‚ºLçš„vector ã€‚
#### Embedding layer
å°‡ input sequence token é€é glove embedding è½‰æ›ç‚º300ç¶­ word vector ã€‚

&emsp;Input: ä¸€å€‹long type çš„é•·åº¦ç‚º$\ L$ çš„vector

&emsp;Output: $\ (L,D_{emb})$

#### bi-LSTM
é¦–å…ˆï¼Œä½¿ç”¨ä¸€å±¤bi-LSTMä¾†åšç‰¹å¾µæå–ï¼Œ$\ h_t$ æ˜¯æ™‚é–“ç‚º$\ t$ æ™‚çš„éš±è—ç‹€æ…‹ï¼ˆè¼¸å‡ºï¼‰ï¼Œ$\ X_0$ æ˜¯ç›®å‰çš„è¼¸å…¥ï¼Œä¹Ÿå°±æ˜¯input sequenceï¼Œ$\ X_T$å‰‡æ˜¯ç›®å‰éš±è—ç¯€é»çš„è¼¸å‡ºï¼Œ

$\ X_T, (h_t, c_t) = LSTM(X_0, (h_0, c_0))$

#### Fully-connected layer
å°‡bi-LSTMè¼¸å‡º$\ h_t$ è¼¸å…¥Fully-connected layerï¼Œå³

$\ y = (y(0), . . . , y(149)) = MLP(concat(h_t[-1],h_t[-2]))$

(å¦‚æœ bidirection = False å‰‡

$\ y = (y(0), . . . , y(149)) = MLP(concat(h_t[-1]))$)

#### *æœ€çµ‚é æ¸¬è¼¸å‡º*
é æ¸¬å°‡æœƒæ˜¯åˆ†æ•¸æœ€é«˜çš„é¡åˆ¥ï¼Œå³

$\ yâˆ— = argmax(y)$

ä¸€äº›bi-LSTM modelçš„è¨­å®šï¼š
1. hidden size: 256
2. num layer: 2
3. dropout: 0.5

ä¸€äº›MLPçš„è¨­å®šï¼š
1. hidden size: 512*512
2. output size: 512*150
3. dropout: 0.5

### b. Performance
* Validation accuracy: 0.905
* Public accuracy: 0.91422

### c. Loss function
æˆ‘ä½¿ç”¨æ¨™æº–[Cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)ï¼Œå®šç¾©$\ y*$ç‚º intent classifier outputï¼Œ$\ gt$ ç‚º ground truth ï¼Œæ‰€ä»¥

$\ Loss = CrossEntropyLoss(y*, ğ‘”ğ‘¡)$

### d. Optimization algorithm, learning rate and batch size etc.
* optimization algorithm: [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) (weight decay=0)
* learning rate: 1e-3 = 0.001
* batch size: 32
* number of epoch: 100
* max lengh: 64


## Q3: Describe your slot tagging model.
### a. model
æˆ‘å°‡æˆ‘çš„æ¨¡å‹å€åˆ†æˆembedding layerã€bi-LSTMã€Fully-connected layer ä¸‰å±¤ã€‚ç¶“éå‰è¿°Data processingçš„tokenizeræœƒè¼¸å‡ºlong type çš„é•·åº¦ç‚ºLçš„vector

#### Embedding layer
å°‡ input sequence token é€é glove embedding è½‰æ›ç‚º300ç¶­ word vector ã€‚

&emsp;Input: ä¸€å€‹long type çš„é•·åº¦ç‚ºLçš„vector

&emsp;Output: $\ (L,D_{emb})$

#### bi-LSTM
é¦–å…ˆï¼Œä½¿ç”¨ä¸€å±¤bi-LSTMä¾†åšç‰¹å¾µæå–ï¼Œ$\ h_t$ æ˜¯æ™‚é–“ç‚º$\ t$ æ™‚çš„éš±è—ç‹€æ…‹ï¼ˆè¼¸å‡ºï¼‰ï¼Œ$\ X_0$ æ˜¯ç›®å‰çš„è¼¸å…¥ï¼Œä¹Ÿå°±æ˜¯input sequenceï¼Œ$\ X_T$å‰‡æ˜¯ç›®å‰éš±è—ç¯€é»çš„è¼¸å‡ºï¼Œ

$\ X_T, (h_t, c_t) = LSTM(X_0, (h_0, c_0))$

#### Fully-connected layer
å°‡bi-LSTMè¼¸å‡º$\ h_t$ è¼¸å…¥Fully-connected layerï¼Œå³

$\ y = (y(0), . . . , y(9)) = MLP(X_T)$

#### *æœ€çµ‚é æ¸¬è¼¸å‡º*
é æ¸¬å°‡æœƒæ˜¯åˆ†æ•¸æœ€é«˜çš„é¡åˆ¥ï¼Œå³

$\ yâˆ— = argmax(y)$

ä¸€äº›bi-LSTM modelçš„è¨­å®šï¼š
1. hidden size: 256
2. num layer: 2
3. dropout: 0.5

ä¸€äº›MLPçš„è¨­å®šï¼š
1. hidden size: 512*512
2. output size: 512*150
3. dropout: 0.5

### b. Performance
* Validation accuracy: 0.817
* Public accuracy: 0.79356


### c. Loss function
æˆ‘ä½¿ç”¨æ¨™æº–[Cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)ï¼Œå®šç¾©$\ y*$ç‚º intent classifier outputï¼Œ$\ gt$ ç‚º ground truth ï¼Œæ‰€ä»¥

$\ Loss = CrossEntropyLoss(y*, ğ‘”ğ‘¡)$

### d. Optimization algorithm, learning rate and batch size etc.
* optimization algorithm: [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) (weight decay=0)
* learning rate: 1e-3 = 0.001
* batch size: 32
* number of epoch: 100
* max lengh: 64


## Q4: Sequence Tagging Evaluation
#### Seqeval
![](https://i.imgur.com/4feIclI.png)

<br><br><br>

* Joint accuracy
    $$    
    \text{Joint accuracy} = \frac{æ­£ç¢ºè¢«é æ¸¬çš„sequence}{æ‰€æœ‰è¢«é æ¸¬sequenceçš„æ•¸é‡}
    $$
* Token accuracy
    $$
    \text{Token accuracy} = \frac{æ­£ç¢ºè¢«é æ¸¬çš„token}{æ‰€æœ‰è¢«é æ¸¬tokençš„æ•¸é‡}
    $$
Seqeval æœƒæŠŠæ¯å€‹ sequence predict/ground truth æ‹†è§£ç‚º(tag, begin, end)çš„åºåˆ—ï¼Œ
> ex.<br>
y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]<br>
y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]<br>
-> y_true = [[(MISC, 3, 6)], [(PER, 0, 2)]]<br>
-> y_pred = [[(MISC, 2, 6)], [(PER. 0, 2)]]

å†å°æ¯å€‹ tag è¨ˆç®— true positive(TP)ï¼Œä¹Ÿå°±æ˜¯æ¯å€‹ squence åŒæ¨£(tag, begin, end)æ•¸é‡ï¼Œ
* $\ Recall$: è¡¨é æ¸¬èƒ½å‘½ä¸­å¤šå°‘çœŸå¯¦çš„æ­£æ¨£æœ¬
    $$
        \text{Recall} = \frac{TP}{TP + FN}
    $$
        
* $\ Precision$: è¡¨é æ¸¬å‡ºç‚ºæ­£æ¨£æœ¬çš„æº–ç¢ºåº¦
    $$
        \text{Precision} = \frac{TP}{TP + F{P}}
    $$
        
* $\ F1-score: ç‚º precision å’Œ recall çš„èª¿å’Œå‡½æ•¸$
    $$
        \text{F1-score} = \frac{2}{\text{Recall}^{-1} + \text{Precision}^{-1}}
    $$
* $\ Support = çœŸå¯¦(tag, begin, end)ç‚ºè©² tag çš„æ•¸é‡$
* $\ Micro avg =$ 
    $$
        \frac{æ‰€æœ‰ tag çš„ TP}{æ‰€æœ‰ tag çš„ TP+FP(æˆ– FN)}
    $$
* $\ Macro avg: å°æ‰€æœ‰çš„ precisionåŠrecall åšå¹³å‡$
* $\ Weighted avg: æ ¹æ“š tag æ•¸é‡åš weighted sum$

## Q5: Compare with different configurations
### 1. hidden size
æˆ‘æƒ³çŸ¥é“hidden sizeå°æº–ç¢ºåº¦çš„å½±éŸ¿ï¼Œæ‰¾å‡ºç›¸å°å¥½çš„hidden sizeèˆ‡epochçµ„åˆã€‚
#### GRU
åˆ†åˆ¥æ¸¬è©¦hidden size ç‚º16ã€64ã€128ã€256ã€512ï¼Œå…¶é¤˜åƒæ•¸è¨­å®šèˆ‡Q2å’ŒQ3å®Œå…¨ä¸€è‡´ã€‚

##### Intent
| Hidden<br>Size | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :------------: | ------------- | ------------- | ------------- | ------------ | ------------ |
|      *16*      | 100            | 1.870003       | 0.3982       | 1.666143      |  0.639333      |
|      *64*      | 86            | 0.179835       |  0.950866       | 1.093286      | 0.891      |
|      *128*     | 36            | 0.137901       | 0.962333       | 0.838733      | 0.902      |
|      *256<sup>*</sup>*     | 46            | 0.113211       | 0.969666       | 0.869564      | 0.907666      |
|      *512*     | 76            | 0.259974       | 0.9338       | 0.908806      | 0.887      |

<font size="2"> *: ç‚ºè©²å¼µè¡¨ä¸­é©—è­‰æº–ç¢ºåº¦æœ€é«˜çš„çµæœ</font>

##### Slot
| Hidden<br>Size | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :------------: | ------------- | ------------- | ------------- | ------------ | ------------ |
|      *16*      | 36            | 0.08825       | 0.380728       | 0.066056      | 0.405      |
|      *64*      | 97            | 0.020173       | 0.701546       | 0.020226      | 0.735      |
|      *128*     | 98            | 0.008617       | 0.833103       | 0.015606      | 0.777   |
|      *256*     | 99            | 0.004206       | 0.897984       | 0.016017      | 0.831      |
|      *512<sup>*</sup>*  | 100            | 0.002134       | 0.947404       | 0.017795      | 0.841 |
<font size="2"> *: ç‚ºè©²å¼µè¡¨ä¸­é©—è­‰æº–ç¢ºåº¦æœ€é«˜çš„çµæœ</font>

#### LSTM
##### Intent
| Hidden<br>Size | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :------------: | ------------- | ------------- | ------------- | ------------ | ------------ |
|      *16*      | 99            | 0.761380       | 0.760266       | 1.406203      | 0.840333      |
|      *64*      | 100           | 0.072918      | 0.980333      | 1.015609      | 0.903666      |
|      *128*     | 86           | 0.073756       | 0.980333       | 0.810521      | 0.91266      |
|      *256<sup>*</sup>*     | 70            | 0.050195       | 0.9996       | 0.720059      | 0.92      |
|      *512*     | 80            | 0.050459       | 0.988333       | 1.127070      | 0.914333 |
<font size="2"> *: ç‚ºè©²å¼µè¡¨ä¸­é©—è­‰æº–ç¢ºåº¦æœ€é«˜çš„çµæœ</font>

##### Slot
| Hidden<br>Size | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :------------: | ------------- | ------------- | ------------- | ------------ | ------------ |
|      *16*      | 100            | 0.067648       | 0.368580       | 0.058628      | 0.39      |
|      *64*      | 99            | 0.021782       | 0.674627       | 0.024262      | 0.715      |
|      *128*      | 97            | 0.0084       | 0.840557       | 0.016649      | 0.802      |
|      *256*      | 95            | 0.004441       | 0.899917       | 0.015021      | 0.828      |
|      *512<sup>*</sup>*      | 81            | 0.002904       | 0.929596       | 0.015282      | 0.83      |
<font size="2"> *: ç‚ºè©²å¼µè¡¨ä¸­é©—è­‰æº–ç¢ºåº¦æœ€é«˜çš„çµæœ</font>

##### å°ç¸½çµ
å°æ–¼Intentçš„è³‡æ–™é›†è€Œè¨€ï¼Œæˆ‘ç™¼ç¾ä¼¼ä¹åˆ°äº†ä¸€å®šç¨‹åº¦å°±æœƒoverfittingï¼Œä¹Ÿå°±æ˜¯ç›²ç›®èª¿é«˜Hidden sizeä¸ä¸€å®šæœƒå–å¾—å¥½çš„çµæœï¼Œå› ç‚ºåœ¨hidden size=512æ™‚ï¼Œé©—è­‰æº–ç¢ºåº¦éƒ½è¼ƒhidden size=256ä¾†å¾—å·®ã€‚slotçš„è³‡æ–™é›†å‰‡é›–hidden sizeè¶Šé«˜ï¼Œé©—è­‰æº–ç¢ºåº¦ä¹Ÿè¶Šé«˜ï¼Œä½†hidden sizeè¶Šå¤§ï¼Œä»£è¡¨æŠ½å‡ºç‰¹å¾µè¶Šå¤šï¼Œå› æ­¤è¨“ç·´æ™‚é–“ä¹Ÿè¼ƒä¹…ã€‚å¦å¤–ï¼Œhidden sizeå¾ˆå°çš„è©±ï¼Œé©—è­‰æº–ç¢ºåº¦ä¹Ÿè¼ƒå·®ï¼Œç”šè‡³å·®ç•°è »å¤§çš„ã€‚

### 2. Number of layers 
æˆ‘å¯¦é©—å°‡GRU/LSTMè¨­å®šä¸åŒå±¤æ•¸ï¼Œåˆ†åˆ¥æ¸¬è©¦1ã€2ã€3å±¤ï¼Œä¾†å°‹æ‰¾ç›¸å°è¼ƒå¥½æº–ç¢ºæ€§çš„çµæœã€‚
#### GRU
##### Intent
æ ¹æ“šå‰é¢Hidden Sizeçš„å¯¦é©—ï¼Œé©—è­‰æº–ç¢ºåº¦(Val. Acc.)è¼ƒé«˜çš„ç‚ºhidden size=256ï¼Œæ‰€ä»¥ä»¥æ­¤ç‚ºåŸºæº–æ¸¬è©¦Number of Layer=1, 2, 3ã€‚
| Number of<br>Layer | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :------------: | ------------- | ------------- | ------------- | ------------ | ------------ |
|      *1*      | 65            | 0.070977       | 0.979666       | 1.067042      |  0.904     |
|      *2<sup>*</sup>*      | 46            | 0.113211       | 0.969666       | 0.869564      | 0.907666   |
|      *3*      | 70            | 0.168817       | 0.952666       | 0.961011      | 0.896      |
<font size="2"> *: ç‚ºè©²å¼µè¡¨ä¸­é©—è­‰æº–ç¢ºåº¦æœ€é«˜çš„çµæœ</font>


##### Slot
æ ¹æ“šå‰é¢Hidden Sizeçš„å¯¦é©—ï¼Œé©—è­‰æº–ç¢ºåº¦(Val. Acc.)è¼ƒé«˜çš„ç‚ºhidden size=512ï¼Œæ‰€ä»¥ä»¥æ­¤ç‚ºåŸºæº–æ¸¬è©¦Number of Layer=1, 2, 3ã€‚
| Number of<br>Layer | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :------------: | ------------- | ------------- | ------------- | ------------ | ------------ |
|      *1*      | 100            | 0.002587       | 0.935118       | 0.017515      |  0.83      |
|      *2<sup>*</sup>*      | 100            | 0.002134       | 0.947404       | 0.017795      | 0.841 |
|      *3*      | 84            | 0.002184       | 0.947266       | 0.016682      | 0.839      |
<font size="2"> *: ç‚ºè©²å¼µè¡¨ä¸­é©—è­‰æº–ç¢ºåº¦æœ€é«˜çš„çµæœ</font>

#### LSTM
##### Intent
æ ¹æ“šå‰é¢Hidden Sizeçš„å¯¦é©—ï¼Œé©—è­‰æº–ç¢ºåº¦(Val. Acc.)è¼ƒé«˜çš„ç‚ºhidden size=256ï¼Œæ‰€ä»¥ä»¥æ­¤ç‚ºåŸºæº–æ¸¬è©¦Number of Layer=1, 2, 3ã€‚
| Number<br>of<br>Layer | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :------------: | ------------- | ------------- | ------------- | ------------ | ------------ |
|      *1*      | 93            | 0.055822       | 0.9882       | 1.055977      |  0.919      |
|      *2<sup>*</sup>*      | 70            | 0.050195       | 0.9996       | 0.720059      | 0.92      |
|      *3*      | 93            | 0.066227       | 0.984866       | 1.02114      | 0.914      |
<font size="2"> *: ç‚ºè©²å¼µè¡¨ä¸­é©—è­‰æº–ç¢ºåº¦æœ€é«˜çš„çµæœ</font>

##### Slot
æ ¹æ“šå‰é¢Hidden Sizeçš„å¯¦é©—ï¼Œé©—è­‰æº–ç¢ºåº¦(Val. Acc.)è¼ƒé«˜çš„ç‚ºhidden size=512ï¼Œæ‰€ä»¥ä»¥æ­¤ç‚ºåŸºæº–æ¸¬è©¦Number of Layer=1, 2, 3ã€‚
| Number of<br>Layer | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :------------: | ------------- | ------------- | ------------- | ------------ | ------------ |
|      *1*      | 94            | 0.002882       | 0.929734       | 0.017374      |  0.822      |
|      *2*      | 81            | 0.002904       | 0.929596       | 0.015282      | 0.83      |
|      *3<sup>*</sup>*      | 84            | 0.002289       | 0.943677       | 0.017507      | 0.833      |
<font size="2"> *: ç‚ºè©²å¼µè¡¨ä¸­é©—è­‰æº–ç¢ºåº¦æœ€é«˜çš„çµæœ</font>

##### å°ç¸½çµ
åœ¨ Intent è³‡æ–™é›†ä¸­ï¼ŒNumber of Layer=2 ç„¡è«–åœ¨LSTMæˆ–æ˜¯GRUéƒ½æœƒæœ‰æ¯”è¼ƒå¥½çš„è¡¨ç¾ï¼›åœ¨ Slot è³‡æ–™é›†ä¸­ï¼ŒNumber of Layer=2 åœ¨ GRU ä¸­æœ‰è¼ƒå¥½çš„è¡¨ç¾ï¼Œåœ¨ LSTM ä¸­å‰‡æ˜¯ Number of Layer=3 æœ‰è¼ƒå¥½è¡¨ç¾ã€‚

## Final model configuration
æ ¹æ“šQ5æ¯”è¼ƒçµæœï¼Œæ•´ç†å‡ºæœ€çµ‚æœ€ä½³è¨­å®šåƒæ•¸ã€‚

### Intent Classifier
| Name | Number of<br>Layer | Hidden<br>Size | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :------------: | :------------: | ------------- | ------------- | ------------- | ------------- | ------------ | ------------ |
| LSTM |      *2<sup>*</sup>*      | 256 | 70            | 0.050195       | 0.9996       | 0.720059      | 0.92      |

##### Performance on Kaggle:
* Public accuracy: 0.92


### Slot Classifier
| Name | Number of<br>Layer | Hidden<br>Size | Best<br>Epoch | Train<br>Loss | Train<br>Acc. | Val.<br>Loss | Val.<br>Acc. |
| :------------: | :------------: | ------------- | ------------- | ------------- | ------------- | ------------ | ------------ |
| GRU |      *2<sup>*</sup>*      | 512 | 100            | 0.002134       | 0.947404       | 0.017795      | 0.841 |

##### Performance on Kaggle:
* Public accuracy: 0.82252

## Reference
* [Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)
* [seqval](https://github.com/chakki-works/seqeval)
